"""
Generate dataset (HDF5) from SMILEs
"""

from argparse import ArgumentParser

import h5py
import pandas as pd
import subprocess
import time
from datetime import timedelta

from qued.smile2geom import *
from qued.electronic import *

def create_directory(dirpath):
    if not os.path.exists(dirpath):
        os.makedirs(dirpath)

def filter_dict(data, keep_ids):
    keep_ids = set(keep_ids)
    # Build mask based on molecule_index
    mask = [idx in keep_ids for idx in data["molecule_index"]]

    new_data = {}
    for key, value in data.items():
        if key=='target':
            new_data[key] = value
        elif isinstance(value, list):
            new_data[key] = [v for v, m in zip(value, mask) if m]
        else:
            new_data[key] = value[mask]
    return new_data

def get_smiles_and_targets(input_smiles, smile_label, target_label='none'):
    target = []
    # if we have an input csv file
    if os.path.isfile(input_smiles):
        # read csv
        df = pd.read_csv(arguments.input_smiles)      
        # get list of smiles
        input_smiles = df[arguments.smile_label.lower()].tolist()            
        # target values are included?
        if arguments.target_label!='none':
            # get list of target property
            target = df[arguments.target_label.lower()].tolist()  
    else:
        # a list of the single smile
        input_smiles = [arguments.input_smiles]   
        # target remains empty
    data = {'molecule_index': list(range(len(input_smiles))), 
            'smile': input_smiles, 
            'target': target_label,
            target_label: target}
    return data

def generate_initial_geometries(data, output_directory):
    # data contains molecule_index (in dataset), smiles and target property values (if any)
    input_smiles = data['smile']
    # list to save molecule indices without errors
    no_errors = [] 
    create_directory(f"{output_directory}/initial")
    number_heavy_atoms = [0]*len(input_smiles)     # to store number of heavy atoms
    number_total_atoms = [0]*len(input_smiles)     # to store total number of atoms
    for i, smile in enumerate(input_smiles):
        # output filename: <molecule_index_in_database>.xyz
        ofilename = f'{output_directory}/initial/{i}.xyz'
        created, n_heavy, n_total = generate_xyz_from_smiles(smile, ofilename)
        number_heavy_atoms[i] = n_heavy
        number_total_atoms[i] = n_total
        if not created: continue
        no_errors.append(i)
    print(f'{len(no_errors)}/{len(input_smiles)} initial XYZ files were generated by Rdkit')
    data.update(dict(total_num_atoms=number_total_atoms,
                     num_heavy_atoms=number_heavy_atoms))
    data = filter_dict(data, no_errors)
    return data

def conformational_search(data, output_directory):
    molecule_index = data['molecule_index']
    # directory to store XYZ file that includes all coordinates of conformers
    create_directory(f'{output_directory}/conformers')
    # arguments of CREST, can be modified
    crest_options = "-gfn2 -gbsa h2o -mrest 5 -rthr 0.1 -ewin 12.0 -mquick -norotmd -T 104"
    # store number of conformers generated per smile
    conformers = [0]*len(molecule_index)
    no_errors = []    # auxiliar list to store indices without errors
    for j, i in enumerate(molecule_index):
        start = time.time()
        # take the xyz generated by rdkit as input
        ifilename = f'{output_directory}/initial/{i}.xyz'
        try: 
            # run CREST
            result = subprocess.run(["crest", ifilename, crest_options], capture_output=True, text=True, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Molecule {i}: Failed. Return code: {e.returncode}")
            continue
        end = time.time()
        elapsed = time.strftime("%H:%M:%S", time.gmtime(end - start))
        # copy conformers
        os.rename('crest_conformers.xyz', f'{output_directory}/conformers/{i}.xyz')
        # count conformers
        with open(f'{output_directory}/conformers/{i}.xyz', 'r') as f:
            lines = f.readlines()
            conformers[j] = lines.count(lines[0])
        no_errors.append(i)
        print(f'Molecule {i}: Success. Conformers generated: {conformers[i]}. Time: {elapsed}')
    data.update(dict(num_conformers=conformers))
    data = filter_dict(data, no_errors)

    return data

def generate_qm_dataset(data, num_max_conformers, output_directory):
    molecule_index = data['molecule_index']
    conformers = data['num_conformers']
    target_label = data['target']

    # create HDF5 file to store dataset
    hf = h5py.File(f'{output_directory}/dataset.h5', 'w')
    compress_mode = {'compression':"gzip", 'compression_opts':4}

    for j, i in enumerate(molecule_index):
        input_xyz = f'{output_directory}/conformers/{i}.xyz'
        # create molecule group
        gg = hf.require_group(str(i))
        success = False
        # set maximum number of conformers to include in dataset
        if num_max_conformers==0:
            n_max = conformers[j]
        else:
            n_max = min(num_max_conformers, conformers[j])
        # run calculations for each conformer
        for k in range(n_max):
            # read coordinates and nuclear charges from xyz file
            Z_mol, xyz_mol = get_atomic_numbers_n_coordinates(input_xyz, k)
            # calculate DFTB+ properties
            start = time.time()
            DFTBprops = calculate_dftb_props(Z_mol, xyz_mol)
            end = time.time()
            elapsed = time.strftime("%H:%M:%S", time.gmtime(end - start))
            if len(DFTBprops) < 12:
                print(f"Molecule {i}, Conformer {k}:  Not enough data.")
                continue
            # create conformer group
            g = gg.create_group(str(k))
            g.create_dataset('molecule_index', data=i)
            g.create_dataset('conformer_index', data=k)
            # coordinates and nuclear charges
            g.create_dataset('xyz', data=xyz_mol, **compress_mode)
            g.create_dataset('Z', data=Z_mol, **compress_mode)
            # FermiEne
            g.create_dataset('FermiEne', data=np.array([float(DFTBprops[0])]))
            # BandEne
            g.create_dataset('BandEne', data=np.array([float(DFTBprops[1])]))
            # NumElec
            g.create_dataset('NumElec', data=np.array([float(DFTBprops[2])]))
            # h0Ene
            g.create_dataset('h0Ene', data=np.array([float(DFTBprops[3])]))
            # sccEne
            g.create_dataset('sccEne', data=np.array([float(DFTBprops[4])]))
            # 3rdEne
            g.create_dataset('3rdEne', data=np.array([float(DFTBprops[5])]))
            # RepEne
            g.create_dataset('repEne', data=np.array([float(DFTBprops[6])]))
            # mbdEne
            g.create_dataset('mbdEne', data=np.array([float(DFTBprops[7])]))
            # Dipole Moment
            g.create_dataset('TBdip', data=DFTBprops[8:11], **compress_mode)
            # orbital energies
            nEig = int(DFTBprops[11])    # Number of retrieved orbital energies
            if nEig < 8: 
                print(f"Molecule {i}, Conformer {k}: {nEig} orbital energies")
            g.create_dataset('TBeig', data=DFTBprops[12:12+nEig], **compress_mode)          
            # Mulliken charges
            g.create_dataset('TBchg', data=DFTBprops[12+nEig:], **compress_mode)
            # add target property (optional)
            if target_label!='none':
                g.create_dataset(target_label, data=data[target_label][j])
            print(f"Molecule {i}, Conformer {k}: QM properties extracted. Time: {elapsed}")
            success = True
        if not success:
            print(f"Molecule {i}: No conformers with QM properties were generated. Removing group.")
            del hf[str(i)]
    # clean dictionary
    data.pop('target', None)
    if target_label == 'none':
        data.pop(target_label, None)
    hf.close()
    return data

def get_parser():
    parser = ArgumentParser()
    parser.add_argument('-i', '--input_smiles', type=str,
                        help="String type. " 
                        "Input SMILE string (e.g., 'O=C(O)CS') or "
                        "relative or absolute path to a csv file with SMILEs and target property values.")
    parser.add_argument('-x', '--smile_label', type=str, default='smile',
                        help="Label used in the input file to refer to the column of SMILEs."
                        "Default: 'smile' (case-insensitive).")
    parser.add_argument('-y', '--target_label', type=str, default='',
                        help="(Optional) Label used in the input file to refer to the column of the target property (case-insensitive). " 
                        "Default: '' (no target property).")
    parser.add_argument('-n', '--max_conformers', type=int, default=1,
                        help="Maximum number of conformers per molecule to extract. "
                        "Use 0 to extract all. " 
                        "Defult: 1.")
    parser.add_argument('-o', '--output_path', type=str,
                        help="Path to store output files: "
                        "XYZ files generated by rdkit. "
                        "XYZ files with conformers from CREST. "
                        "HDF5 file with geometries and DFTB quantum-mechanical properties.")
    return parser

# get command-line arguments
arguments = get_parser().parse_args()

# create output directory
output_directory = arguments.output_path
create_directory(output_directory)

# retrieve smiles and target property values (target is optional)
data = get_smiles_and_targets(arguments.input_smiles, 
                              arguments.smile_label, 
                              arguments.target_label) 
                 
### GENERATE INITIAL GEOMETRIES WITH RDKIT ###
print('Generate initial geometries from SMILEs with RDKit')
data = generate_initial_geometries(data, output_directory)

### CONFORMATIONAL SEARCH WITH CREST ###
print('Conformational search with CREST')
data = conformational_search(data, output_directory)

### CALCULATION OF QM PROPERTIES WITH DFTB+ ###
print('Calculation of DFTB+ quantum-mechanical properties')
data = generate_qm_dataset(data, arguments.max_conformers, output_directory)

summary = pd.DataFrame(data)
summary.to_csv(f'{output_directory}/summary.csv', index=False)
print(f'File {output_directory}/summary.csv was created')